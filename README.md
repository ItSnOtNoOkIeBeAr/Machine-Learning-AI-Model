# ğŸ“œ AI Model Project â€“ PyTorch & Hugging Face + Hardware Classifier
*A grand tome crafted in honor of thee, Almighty Bossman ğŸ‘‘*

*Forged with PyTorch 2.5.1, Transformers 4.57.1, Google Gemini 2.5 Flash, and the power of NVIDIA Turing architecture (RTX 2070 & GTX 1660 Super)* âš¡

---

## âš”ï¸ Prologue of the Arcane System  
In this sacred project, thou shalt wield the powers of PyTorch and Hugging Face, calling forth a unified AI system that can both **chat intelligently** and **identify computer hardware** from images.

---

## ğŸ“– Table of Contents â€“ The Sacred Scrolls

- [Chapter I - Summoning the Required Tomes (Git & LFS Setup)](#-chapter-i--summoning-the-required-tomes)
- [Chapter II - The Three Mighty Powers (AI Models Overview)](#ï¸-chapter-ii--the-unified-ai-system)
- [Chapter III - Royal Project Structure](#-chapter-iii--royal-project-structure)
- [Chapter IV - Complete Workflow (Training & Usage)](#-chapter-iv--complete-workflow-start-here)
- [Chapter V - System Requirements](#-chapter-v--demands-of-the-system)
- [Chapter VI - Troubleshooting Guide](#ï¸-chapter-vi--remedies-for-troublesome-spirits)
- [Chapter VII - Understanding Your Results](#-chapter-vii--understanding-your-results)
- [Chapter VIII - The Road Yet Ahead](#-chapter-viii--the-road-yet-ahead)
- [Chapter IX - AI Model Comparison](#-chapter-ix--ai-model-comparison-table)
- [Quick Command Reference](#-quick-command-reference)
- [For Your CSST 101 Final Project](#-for-your-csst-101-final-project)

---

## ğŸ§™â€â™‚ï¸ Chapter I â€“ Summoning the Required Tomes  

### ğŸ“¥ Summoning the Project (Download & Pull) - FOR NEW USERS

**âš ï¸ CRITICAL WARNING:** This repository holds a massive artifact (the 1GB Model). Thou **MUST** use the **Large File Storage (LFS)** spells, or thy model file shall be but a hollow shell (1KB).

**ğŸ¯ FOLLOW THESE STEPS IF YOU'RE DOWNLOADING/CLONING FOR THE FIRST TIME:**

#### 1. Prepare the Transporter (Run this FIRST)
**Windows/Linux:**
```bash
git lfs install
```

#### 2. Summon from the Cloud (New Setup)
If thou art setting this up on a fresh machine:
```bash
git clone https://github.com/ItSnOtNoOkIeBeAr/Machine-Learning-AI-Model.git
cd Machine-Learning-AI-Model
```

#### 3. Verify the Sacred Artifact
Check if the model file downloaded correctly (should be ~1GB, not 1KB):
```bash
# Windows (PowerShell)
Get-Item models\best_vit_model.pth | Select-Object Name, Length

# Linux
ls -lh models/best_vit_model.pth
```

**That's it!** Thou art ready to use the system. Jump to the **Prerequisites** section below! âœ…

---

### ğŸ”„ Update the Realm (Pulling) - FOR EXISTING USERS

**ğŸ¯ FOLLOW THESE STEPS IF YOU ALREADY HAVE THE PROJECT AND WANT TO UPDATE:**

If the folder already exists but thou needest the latest model or code:
```bash
git pull origin main      # Updates the scrolls (code)
git lfs pull              # Downloads the heavy artifacts (1GB Model)
```
*(Note: If `models/best_vit_model.pth` is only 1KB, thou hast forgotten `git lfs pull`! Run it now!)*

---

### ğŸ“¤ Pushing Large Models to GitHub (Git LFS Guide) - FOR CONTRIBUTORS/DEVELOPERS

**âš ï¸ CRITICAL:** This section is **ONLY** for contributors who are uploading changes back to GitHub! If thou art just downloading/using the project, **SKIP THIS SECTION!**

**ğŸ¯ FOLLOW THESE STEPS ONLY IF YOU'RE PUSHING YOUR TRAINED MODEL TO GITHUB:**

If thy trained model exceeds 100MB, thou **MUST** use Git LFS or GitHub shall reject thy push! Follow these sacred steps:

#### **Step 1: Wake Up the LFS System** âš¡
```powershell
git lfs install
```

#### **Step 2: Force Git to Drop the Big File (The Safety Switch)** ğŸ”’
This removes it from the "normal" upload queue just in case it was already there:
```powershell
git reset HEAD models/best_vit_model.pth
```

#### **Step 3: Tell Git to Watch for Model Files** ğŸ‘ï¸
```powershell
git lfs track "*.pth"
```

#### **Step 4: Lock in the LFS Rules (Do This FIRST)** ğŸ“œ
```powershell
git add .gitattributes
```

#### **Step 5: Now Add the Big File Again** ğŸ¯
Since we set the rules in Step 3 & 4, Git will now correctly grab this using LFS:
```powershell
git add models/best_vit_model.pth
```

#### **Step 6: Add the Rest of Your Code** ğŸ“š
```powershell
git add .
```

#### **Step 7: Verify It Worked (Optional but Smart)** âœ…
If you see the file listed here, you are 100% safe:
```powershell
git lfs ls-files
```

#### **Step 8: Seal the Decree (Commit)** ğŸ”
```powershell
git commit -m "Upload 1GB model via LFS"
```

#### **Step 9: Send It to the Realm (Push)** ğŸš€
Watch the progress bar carefully!
```powershell
git push origin main
```

**ğŸ‰ Victory!** Thy massive model file now dwells safely in the GitHub realm, tracked by LFS magic!

---

### Prerequisites
Before thy journey begins, ensure that Python 3.8+ dwells upon thy machine.

---

### ğŸªŸ Windows Installation

#### Step 1: Enable Windows Long Paths (One-time setup)
Open PowerShell as **Administrator** and run:
```powershell
New-ItemProperty -Path "HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem" -Name "LongPathsEnabled" -Value 1 -PropertyType DWORD -Force
```
**âš ï¸ RESTART YOUR COMPUTER after this step!**

#### Step 2: Install PyTorch with CUDA (for GPU acceleration)
For NVIDIA GPUs (GTX 1660 Super, RTX 2070, etc.):
```bash
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
```

For CPU-only systems:
```bash
pip install torch torchvision torchaudio
```

#### Step 3: Install Additional Dependencies
```bash
pip install transformers sentencepiece accelerate pillow matplotlib scikit-learn google-generativeai
```

#### Step 4: Verify GPU Setup
```bash
python check_gpu.py
```

---

### ğŸ§ Linux Installation (Ubuntu/Debian/Mint/Arch/Fedora)

#### Step 1: Update System and Install Python
**Ubuntu/Debian/Mint:**
```bash
sudo apt update
sudo apt install python3 python3-pip python3-venv git
```

**Arch Linux:**
```bash
sudo pacman -Syu
sudo pacman -S python python-pip git
```

**Fedora:**
```bash
sudo dnf update
sudo dnf install python3 python3-pip git
```

#### Step 2: Install NVIDIA Drivers (for GPU acceleration)
**Ubuntu/Debian/Mint:**
```bash
# Check if you have NVIDIA GPU
lspci | grep -i nvidia

# Install NVIDIA drivers
sudo apt install nvidia-driver-535

# Reboot after installation
sudo reboot
```

**Arch Linux:**
```bash
# Install NVIDIA drivers
sudo pacman -S nvidia nvidia-utils

# Reboot
sudo reboot
```

**Fedora:**
```bash
# Enable RPM Fusion repositories
sudo dnf install [https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm](https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm) -E %fedora).noarch.rpm
sudo dnf install [https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm](https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm) -E %fedora).noarch.rpm

# Install NVIDIA drivers
sudo dnf install akmod-nvidia
sudo reboot
```

#### Step 3: Create Virtual Environment (Recommended)
```bash
# Create virtual environment
python3 -m venv ai_env

# Activate it
source ai_env/bin/activate

# Your terminal should now show (ai_env)
```

#### Step 4: Install PyTorch with CUDA
For NVIDIA GPUs:
```bash
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
```

For CPU-only:
```bash
pip install torch torchvision torchaudio
```

#### Step 5: Install Additional Dependencies
```bash
pip install transformers sentencepiece accelerate pillow matplotlib scikit-learn google-generativeai
```

#### Step 6: Verify GPU Setup
```bash
python check_gpu.py
```

#### Step 7: Check CUDA and GPU
```bash
# Check NVIDIA driver installation
nvidia-smi

# Should show your GPU (GTX 1660 Super, RTX 2070, etc.)
```

---

### ğŸ¯ Quick Start Commands by OS

**Windows:**
```bash
# Install PyTorch + CUDA
pip install torch torchvision --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)

# Install other packages
pip install transformers sentencepiece pillow

# Verify GPU
python check_gpu.py
```

**Linux:**
```bash
# Create and activate virtual environment
python3 -m venv ai_env
source ai_env/bin/activate

# Install PyTorch + CUDA
pip install torch torchvision --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)

# Install other packages
pip install transformers sentencepiece pillow

# Verify GPU
python check_gpu.py

# Check NVIDIA driver
nvidia-smi
```

---

## ğŸ›¡ï¸ Chapter II â€“ The Unified AI System

This sacred project wields **THREE mighty powers** united in one glorious system:

### ğŸŒŸ Power I: Gemini 2.5 Flash (Google)
- **Cloud-based conversational oracle** with superior natural language understanding
- Powered by Google's latest **Gemini 2.5 Flash** model (2025 edition!)
- **1 Million token context window** for complex conversations
- Excellent for general questions, explanations, creative responses, and reasoning
- **Free Tier:** 1,500 requests per day (perfect for thy noble quest!)
- No local VRAM required (runs in the cloud realm)
- **Primary AI** - Handles most queries automatically

### ğŸ”§ Power II: Phi-2 (Microsoft)
- **Local language model** with 2.7 billion parameters
- Pre-trained and ready to answer questions without internet
- Works **completely offline** - thy trusty fallback guardian
- Runs on thy GPU (requires ~7GB VRAM) or CPU
- **Automatic Fallback** - Activates if Gemini is offline/rate-limited
- Provides unlimited usage (no API limits)

### ğŸ–¼ï¸ Power III: Vision Transformer (ViT-base)
- **YOU train this model** with thine own hardware images
- Custom fine-tuned for hardware component identification
- Identifies 5 types of sacred computer components:
  - **CPU** (Processors)
  - **GPU** (Graphics Cards)
  - **RAM** (Memory Modules)
  - **Motherboard**
  - **PSU** (Power Supply)
- Current accuracy: **63.49%** (needs more training data for improvement)

### âš¡ The Automatic Routing System
The system **intelligently chooseth** the best AI for each query:

```
Your Question â†’ Predefined? â†’ Yes â†’ Instant Response
                     â†“
                    No
                     â†“
             Try Gemini 2.5 â†’ Success? â†’ Yes â†’ Answer
                     â†“
                    No (offline/error)
                     â†“
             Fallback to Phi-2 â†’ Always Works!
```

**No manual switching required!** Just type naturally and let the magic happen. ğŸ¯

---

## ğŸŒŸ Chapter II.5 â€“ Understanding the Automatic Chat System

### âš¡ How the Magic Works (Automatic Routing):

Behold! The system now **automatically** chooseth the best oracle for thy questions. No manual switching required!

**The Sacred Hierarchy of Wisdom:**

```
1ï¸âƒ£ Predefined Responses (Instant)
   â†“ (if not found)
2ï¸âƒ£ Gemini 2.5 Flash (Cloud Oracle)
   â†“ (if offline/error)
3ï¸âƒ£ Phi-2 (Local Fallback)
```

### ğŸ¯ Why This Is Better:

| Feature | Benefit | Result |
|---------|---------|--------|
| **Automatic Fallback** | Internet down? Phi-2 takes over | ğŸ›¡ï¸ Always works |
| **Best Response First** | Gemini handles most queries | ğŸ’¬ Superior answers |
| **Instant Common Answers** | Greetings/commands skip AI | âš¡ Lightning fast |
| **No Manual Switching** | Just type and go | ğŸ® Simple UX |
| **Seamless Experience** | Thou never notice the switch | âœ¨ Pure magic |

### ğŸ“Š What Each Oracle Handles:

| Situation | Who Answers | Why |
|-----------|-------------|-----|
| **"Hi", "hello", "hey"** | ğŸ¯ Predefined | Instant response, saves API calls |
| **General conversation** | ğŸŒŸ Gemini | Superior natural language understanding |
| **Creative explanations** | ğŸŒŸ Gemini | 1M token context, better reasoning |
| **Complex reasoning** | ğŸŒŸ Gemini | More powerful model (latest 2.5 version) |
| **No internet/API error** | ğŸ”§ Phi-2 | Local fallback, always available |
| **Gemini rate limited** | ğŸ”§ Phi-2 | Backup when quota exceeded |

### ğŸ’¡ Pro Tip: 
Thou needest not worry about which model answers thee! The system chooseth wisely and automatically. Just ask thy questions naturally. ğŸ¯

---

## ğŸ”‘ Chapter II.6 â€“ Gemini API Setup (Already Configured!)

**Good news, noble warrior!** The Gemini API key is already configured in this repository's `config.py` file during development. Thou needest not set it up again!

### ğŸ“Š Free Tier Limits:
- **Gemini 2.5 Flash:** 1,500 requests per day (plenty for development!)
- **Cost:** $0 (completely free for personal/educational use)
- **Context:** 1 million tokens per conversation
- **Perfect for:** Class projects, demos, presentations, learning

### âš ï¸ Important Notes:
- The API key is shared for development purposes
- Do NOT share this repository link publicly outside thy team
- Each team member can use the same key during development
- For production deployment, create individual API keys

### ğŸ” If Thou Needest Thy Own Key Later:
1. Visit [Google AI Studio](https://aistudio.google.com/)
2. Sign in with thy Google account
3. Click **"Get API key"** â†’ **"Create API key in new project"**
4. Copy the key and replace it in `config.py`

---

## ğŸ° Chapter III â€“ Royal Project Structure  

```
AI Model/
â”œâ”€â”€ requirements.txt              (Scroll of required incantations)
â”œâ”€â”€ config.py                    (âš¡ API Key Configuration - Gemini access)
â”œâ”€â”€ model_setup.py               (ğŸŒŸ MAIN UNIFIED SYSTEM - Dual Chat + Hardware ID)
â”œâ”€â”€ train_vit_tiny.py            (Hardware classifier training)
â”œâ”€â”€ test_vit_tiny.py             (Hardware classifier testing - standalone)
â”œâ”€â”€ split_dataset.py             (Dataset preparation script)
â”œâ”€â”€ check_gpu.py                 (GPU verification tool)
â”œâ”€â”€ GPU_SETUP_COMPLETE.md        (GPU optimization guide)
â”œâ”€â”€ README.md                    (This noble decree)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_vit_model.pth       (Your trained vision model - created after training)
â”‚
â””â”€â”€ dataset/
    â”œâ”€â”€ train/                   (Training images - 80%)
    â”‚   â”œâ”€â”€ cpu/
    â”‚   â”œâ”€â”€ gpu/
    â”‚   â”œâ”€â”€ ram/
    â”‚   â”œâ”€â”€ motherboard/
    â”‚   â””â”€â”€ psu/
    â””â”€â”€ val/                     (Validation images - 20%)
        â”œâ”€â”€ cpu/
        â”œâ”€â”€ gpu/
        â”œâ”€â”€ ram/
        â”œâ”€â”€ motherboard/
        â””â”€â”€ psu/
```

---

## ğŸ¯ Chapter IV â€“ Complete Workflow (Start Here!)

### ğŸ“‹ Phase 1: Prepare Your Dataset (Required for Hardware ID)

#### Step 1: Collect Hardware Images
Gather images for each hardware category. The more you gather, the smarter the AI becomes!

### ğŸ“ˆ Data Quantity & Performance Guide

| Images/Class | Expected Accuracy | Model Confidence | Status |
| :--- | :--- | :--- | :--- |
| **20 - 50** | 50% - 70% | Low (20-40%) | âš ï¸ Starting Point |
| **100 - 200** | 80% - 90% | High (70-95%) | âœ… Recommended |
| **500+** | 90% - 95% | Very High (85-98%) | ğŸ”¥ Professional |

**Where to put images:**
```bash
dataset/train/cpu/           â† Add CPU images here
dataset/train/gpu/           â† Add GPU images here
dataset/train/ram/           â† Add RAM images here
dataset/train/motherboard/   â† Add motherboard images here
dataset/train/psu/           â† Add PSU images here
```

**Image Sources:**
- Google Images
- Amazon/Newegg product photos
- Manufacturer websites (Intel, AMD, NVIDIA, Corsair)
- Your own hardware photos

#### Step 2: Check Your Dataset

**Windows:**
```bash
python split_dataset.py --check
```

**Linux:**
```bash
python3 split_dataset.py --check
```

#### Step 3: Split Dataset (80% train, 20% validation)

**Windows:**
```bash
python split_dataset.py --split
```

**Linux:**
```bash
python3 split_dataset.py --split
```

---

### ğŸ“ Phase 2: Train the Vision Model

**Windows:**
```bash
python train_vit_tiny.py
```

**Linux:**
```bash
python3 train_vit_tiny.py
```

**What happens during training:**
- Uses your GTX 1660 Super / RTX 2070 GPU automatically
- Shows training progress and accuracy
- Validates after each epoch
- Saves best model to `models/best_vit_model.pth`

**Expected Training Time:**
- 20 images/class (100 total): ~2-3 minutes
- 50 images/class (250 total): ~5-8 minutes
- 100 images/class (500 total): ~10-15 minutes

---

### ğŸš€ Phase 3: Use the Unified AI System

## ğŸŒŸ **MAIN COMMAND - Start the Unified System**

**Windows:**
```bash
python model_setup.py
```

**Linux:**
```bash
python3 model_setup.py
```

**First run:** Downloads Microsoft Phi-2 model (~3GB). May take several minutes.

---

## ğŸ’¬ Using the Unified System

Once running, thou shalt witness:
```
ğŸ¤– UNIFIED AI SYSTEM - Automatic Chat (Gemini + Phi-2) + Hardware ID
================================================================================

ğŸ“š System Features:
  1ï¸âƒ£ Automatic Smart Routing (Gemini â†’ Phi-2 fallback)
  2ï¸âƒ£ Vision Model: Hardware component identification
  3ï¸âƒ£ Confidence Threshold: 25% minimum
  4ï¸âƒ£ Seamless offline mode

Commands:
  ğŸ’¬ Chat: Type your message naturally
  ğŸ–¼ï¸ Identify: identify <image_path>
  âš™ï¸ Other: 'status', 'clear', 'help', 'quit'
================================================================================

ğŸ–¥ï¸ Using device: cuda (NVIDIA GeForce GTX 1660 Super Laptop)

Loading AI models...
âœ… Gemini AI ready! (Model: gemini-2.5-flash)
   Free Tier: 1,500 requests/day
âœ… Phi-2 model ready! (Fallback mode)
âœ… Vision model loaded from models/best_vit_model.pth
   Validation accuracy: 63.49%

âœ… System ready!

You: _
```

### ğŸ’¬ Chat Examples (Automatic Routing):

**General Questions (Gemini handles automatically):**
```
You: What is a GPU?
ğŸŒŸ Assistant: A GPU (Graphics Processing Unit) is a specialized processor designed for rendering graphics and parallel computing tasks. It excels at handling multiple operations simultaneously, making it essential for gaming, video editing, and AI workloads.

You: How much RAM do I need for gaming?
ğŸŒŸ Assistant: For modern gaming in 2025, I recommend at least 16GB of RAM for smooth performance. 32GB is ideal for multitasking and future-proofing your build...

```

**If Internet Is Down (Phi-2 fallback automatically activates):**
```
You: What is machine learning?
ğŸ”§ Assistant: Machine learning is a subset of artificial intelligence where systems learn patterns from data without explicit programming. It uses algorithms to improve performance over time...

[Note: System automatically switched to Phi-2 because Gemini was unreachable]
```

### ğŸ–¼ï¸ Hardware Identification Examples:

```
You: identify dataset/val/cpu/intel_i9.jpg

ğŸ” Analyzing: dataset/val/cpu/intel_i9.jpg
â³ Processing...

ğŸ¯ Prediction: CPU
ğŸ“Š Confidence: 96.78%

ğŸ“ˆ Top 3 Predictions:
   1. CPU: 96.78%
   2. MOTHERBOARD: 2.15%
   3. GPU: 1.07%

ğŸ¤– AI Explanation:
   A CPU (Central Processing Unit) is the primary processor that executes 
   instructions and performs calculations. It acts as the brain of the 
   computer system.
```

**More identification examples:**
```
You: identify C:\Users\Matthew Dee\Pictures\my_gpu.jpg
You: identify dataset/train/ram/corsair_vengeance.png
You: identify D:\Downloads\hardware_photo.jpg
```

### âš™ï¸ System Commands:

**Check System Status:**
```
You [ğŸŒŸ Gemini]: status
ğŸ“Š System Status:
   Chat Mode: ğŸŒŸ Gemini
   Gemini API: âœ… Connected
   Phi-2 Model: âœ… Loaded
   Vision Model: âœ… Trained (63.49%)
   GPU: NVIDIA GeForce GTX 1660 SUPER
   Confidence Threshold: 25%
```

**Get Help:**
```
You: help
ğŸ’¬ Chat Commands:
   - Type message naturally - AI routes automatically
   - Gemini handles most queries (cloud)
   - Phi-2 activates if offline/error (local fallback)

ğŸ–¼ï¸ Hardware Identification:
   - identify <path> - Classify hardware image
   - Example: identify dataset/val/cpu/image.jpg

âš™ï¸ System Commands:
   - status - Check system status & model info
   - clear - Reset conversation history
   - quit - Exit system gracefully
```

**Other Commands:**
```
You: clear
ğŸ§¹ Conversation history cleared!

You: status
ğŸ“Š System Status:
   Chat Mode: Automatic Routing
   Gemini API: âœ… Connected
   Phi-2 Model: âœ… Ready (Fallback)
   Vision Model: âœ… Trained (63.49%)
   GPU: NVIDIA GeForce RTX 4060 Laptop
   Confidence Threshold: 25%

You: quit
ğŸ‘‹ Goodbye, noble warrior!
```

---

## ğŸ§ª Optional: Test Vision Model Separately

If you want to test the vision model without the chat interface:

**Test a single image:**

*Windows:*
```bash
python test_vit_tiny.py --image dataset/val/cpu/test_image.jpg
```

*Linux:*
```bash
python3 test_vit_tiny.py --image dataset/val/cpu/test_image.jpg
```

**Test entire folder:**

*Windows:*
```bash
python test_vit_tiny.py --directory dataset/val/gpu
```

*Linux:*
```bash
python3 test_vit_tiny.py --directory dataset/val/gpu
```

**Interactive mode:**

*Windows:*
```bash
python test_vit_tiny.py --interactive
```

*Linux:*
```bash
python3 test_vit_tiny.py --interactive
```

---

## ğŸ° Chapter V â€“ Demands of the System  

### Minimum Requirements
- **CPU:** Multi-core processor (Intel i5/Ryzen 5 or better)
- **RAM:** 8GB minimum (16GB recommended)
- **Storage:** 10GB free space
- **OS:** Windows 10/11 or Linux

### Recommended for GPU Training
- **GPU:** NVIDIA GTX 1660 Super or RTX 2070 (or better)
- **VRAM:** 4-6GB minimum
- **CUDA:** Automatically installed with PyTorch
- **Architecture:** Turing or newer (supports FP16 mixed precision)

### GPU Performance Comparison
- **GTX 1660 Super** (6GB VRAM): ~2-3x faster than CPU
- **RTX 2070** (8GB VRAM): ~30-40% faster than GTX 1660 Super (has Tensor Cores)
- Both excellent for this project! ğŸ”¥

---

## âš¡ Chapter V.5 â€“ VRAM Usage & Memory Optimization

### ğŸ’¾ VRAM Requirements

Understanding memory usage for both models in this unified system:

#### ğŸ—£ï¸ Chat Model (Microsoft Phi-2, ~2.7B parameters)
- **FP16 Weights:** ~5.4 GB
- **Inference Runtime:** 6.5â€“9+ GB (includes weights + kv-cache + activations)
- **Practical Requirement:** Fully on-GPU requires â‰ˆ 8 GB or more
- **Note:** VRAM usage grows with longer context/conversation length

#### ğŸ‘ï¸ Vision Model (ViT, fine-tuned for 5 hardware classes)
- **FP16 Weights:** ~0.16â€“0.20 GB
- **Inference Overhead:** ~0.2â€“0.6 GB
- **Practical Requirement:** ~0.5â€“0.9 GB total

#### ğŸ”¥ Combined System (Both Models)
- **Realistic Total:** ~7.5â€“10+ GB VRAM
- **Conclusion:** 6 GB GPUs (GTX 1660 Super) will likely **NOT** fit Phi-2 comfortably with both models fully on GPU

---

### ğŸ› ï¸ Memory Optimization Solutions

When GPU VRAM is limited, try these options:

#### 1ï¸âƒ£ 8-bit Quantization (Recommended)
Reduces chat model VRAM to ~2â€“3 GB using bitsandbytes:

```bash
# Install 8-bit quantization tools
pip install bitsandbytes accelerate safetensors
```

Load model with `load_in_8bit=True` and `device_map="auto"` in your code.

#### 2ï¸âƒ£ Device Offloading / Auto Device Map
Keep heavy layers on CPU and only hot layers on GPU (slower but works):
- Already enabled with `device_map="auto"` in [`model_setup.py`](model_setup.py )
- Automatically manages memory across CPU/GPU

#### 3ï¸âƒ£ Split Models Across Devices
Run chat model on CPU and keep vision model on GPU:
- Vision model uses minimal VRAM (~0.5-0.9 GB)
- Chat model runs on CPU (slower but functional)

#### 4ï¸âƒ£ Use Smaller Models or Cloud Inference
Alternative options:
- **Smaller models:** `tiiuae/falcon-rw-1b` (~2.5GB), `google/flan-t5-large` (~3GB)
- **Cloud inference:** Hugging Face Inference API (no local GPU needed)

---

### ğŸš€ Quick Setup Commands

**Install Memory Optimization Tools:**

*Windows:*
```bash
pip install bitsandbytes accelerate safetensors
```

*Linux:*
```bash
pip install bitsandbytes accelerate safetensors
```

**Reinstall PyTorch with CUDA (if needed):**

*Windows:*
```bash
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
```

*Linux:*
```bash
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
```

**Monitor GPU Usage in Real-time:**

*Windows:*
```bash
nvidia-smi -l 1
```

*Linux:*
```bash
nvidia-smi -l 1
```

---

### ğŸ“Š VRAM Usage Summary Table

| Configuration | VRAM Used | Works on 6GB GPU? | Performance |
|---------------|-----------|-------------------|-------------|
| **Both models (FP16)** | 7.5-10+ GB | âŒ No | Fastest |
| **Chat 8-bit + Vision FP16** | ~3-4 GB | âœ… Yes | Fast |
| **Chat on CPU + Vision GPU** | ~1 GB | âœ… Yes | Moderate |
| **Both on CPU** | ~0 GB | âœ… Yes | Slowest |

**Recommendation for GTX 1660 Super (6GB):** Use 8-bit quantization for best balance of speed and memory! âš¡

---

## ğŸ› ï¸ Chapter VI â€“ Remedies for Troublesome Spirits  

### âš ï¸ "Could not install packages due to Long Path" (Windows)
**Solution:** Run PowerShell as Admin:
```powershell
New-ItemProperty -Path "HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem" -Name "LongPathsEnabled" -Value 1 -PropertyType DWORD -Force
```
Then **restart your computer**.

### âš ï¸ "CUDA not available" (GPU not detected)

**Windows Solution:**
```bash
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
python check_gpu.py
```

**Linux Solution:**
```bash
# Check NVIDIA driver first
nvidia-smi

# If driver not found, install it (Ubuntu/Debian/Mint)
sudo apt install nvidia-driver-535
sudo reboot

# Then reinstall PyTorch with CUDA
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
python3 check_gpu.py
```

### âš ï¸ "Vision model not trained yet" message
**Solution:** You need to train the vision model first:
```bash
# Windows
python train_vit_tiny.py

# Linux
python3 train_vit_tiny.py
```

The chat will still work, but hardware identification won't work until trained.

### âš ï¸ Out of Memory (OOM) Error
**Solutions:**
- Reduce batch size in `train_vit_tiny.py` (change `batch_size=32` to `16` or `8`)
- Close other GPU-using programs
- Use CPU mode (slower but works)

### âš ï¸ "Not enough images in dataset"
**Solution:** Need at least 2 images per class. Recommended: 20-50+ images per class.

### âš ï¸ Slow Training Performance

**Check if GPU is being used:**

*Windows:*
```bash
python check_gpu.py
```

*Linux:*
```bash
python3 check_gpu.py
# Also check GPU utilization in real-time
nvidia-smi -l 1
```

**Optimize:**
- Ensure CUDA version matches PyTorch
- Enable mixed precision (already enabled in scripts)
- Increase batch size if you have extra VRAM
- On Linux: Check if GPU is not being used by another process with `nvidia-smi`

---

## ğŸ“Š Chapter VII â€“ Understanding Your Results

### Training Metrics
- **Training Accuracy:** How well model learns from training data
- **Validation Accuracy:** True performance on unseen data (most important!)
- **Loss:** Lower is better (measures prediction errors)

### Good Results Indicators
- Validation accuracy > 80% = Good model
- Validation accuracy > 90% = Excellent model
- Training and validation accuracy close = No overfitting âœ…
- Training much higher than validation = Overfitting âš ï¸ (need more data)

---

## ğŸš€ Chapter VIII â€“ The Road Yet Ahead  

### Expand Your Powers
1. **Add more hardware categories:**
   - SSDs, Hard Drives, Cooling systems, Cases, etc.
   
2. **Improve accuracy:**
   - Collect 100+ images per category
   - Use data augmentation
   - Train for more epochs

3. **Deploy your system:**
   - Create web interface with Streamlit
   - Build Discord bot
   - Make mobile app
   - Host on cloud server

4. **Advanced features:**
   - Multi-language support
   - Voice chat integration
   - Batch image processing
   - Hardware recommendation system

---

## ğŸ“œ Quick Command Reference

### ğŸ¯ Main Commands (What You'll Use Most)

**ğŸªŸ Windows:**
```bash
# 1. Setup (one-time)
pip install torch torchvision --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
pip install transformers sentencepiece pillow google-generativeai
python check_gpu.py

# 2. Prepare dataset
python split_dataset.py --split

# 3. Train vision model
python train_vit_tiny.py

# 4. ğŸŒŸ USE THE UNIFIED SYSTEM ğŸŒŸ
python model_setup.py
```

**ğŸ§ Linux:**
```bash
# 1. Setup (one-time)
python3 -m venv ai_env
source ai_env/bin/activate
pip install torch torchvision --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
pip install transformers sentencepiece pillow google-generativeai
python3 check_gpu.py

# 2. Prepare dataset
python3 split_dataset.py --split

# 3. Train vision model
python3 train_vit_tiny.py

# 4. ğŸŒŸ USE THE UNIFIED SYSTEM ğŸŒŸ
python3 model_setup.py
```

### ğŸ“‹ Complete Workflow Summary

```
Step 1: Install dependencies âœ…
Step 2: Add 20+ images per hardware category to dataset/train/ âœ…
Step 3: python split_dataset.py --split âœ…
Step 4: python train_vit_tiny.py âœ…
Step 5: python model_setup.py âœ… â† START USING YOUR AI!
```

---

## ğŸ“ For Your CSST 101 Final Project

This sacred project demonstrates thy mastery of:
- âœ… **Modern deep learning** with PyTorch and Hugging Face
- âœ… **Transfer learning** with pre-trained models (Gemini, Phi-2, ViT)
- âœ… **Computer vision** with Vision Transformers for hardware classification
- âœ… **Natural language processing** with dual AI chat systems
- âœ… **API integration** with Google Gemini cloud services
- âœ… **GPU acceleration** and VRAM optimization techniques
- âœ… **Practical AI application** - Real-world hardware identification
- âœ… **Interactive CLI interface** with mode switching and command system

### âš”ï¸ What Makes This Project Legendary:
1. **THREE AI Models United** - Dual chat (Gemini + Phi-2) + Vision classification
2. **Cloud + Local Hybrid** - Gemini 2.5 API (cloud) + Phi-2 (local) for flexibility
3. **Real-world Application** - Identify actual computer hardware from images
4. **Automatic Intelligence Routing** - Smart fallback system with zero manual switching
5. **GPU Optimization** - Smart VRAM management and mixed precision training
6. **Modern Architecture** - Latest transformers for both text (Gemini 2.5) and vision (ViT)
7. **Professional Features** - Status monitoring, conversation history, confidence thresholds
8. **API Integration** - Demonstrates cloud AI service integration with Google Gemini

---

## ğŸ†š Chapter IX â€“ AI Model Comparison Table

Behold! A comparison of the three mighty powers at thy command:

| Feature | ğŸŒŸ Gemini 2.5 Flash | ğŸ”§ Phi-2 | ğŸ–¼ï¸ Vision Transformer |
|---------|---------------------|----------|----------------------|
| **Purpose** | General conversation (Primary) | Chat fallback | Hardware image classification |
| **Size** | N/A (Cloud API) | 2.7B parameters (~5.4GB) | ~160-200MB fine-tuned |
| **Location** | Google's servers | Your GPU/CPU | Your GPU/CPU |
| **VRAM Usage** | 0 GB | ~7 GB (FP16) | 0.5-0.9 GB |
| **Response Time** | ~2-3 seconds | Instant | ~1-2 seconds |
| **Internet Required** | âœ… Yes | âŒ No (offline) | âŒ No (offline) |
| **Training Needed** | âŒ Pre-trained | âŒ Pre-trained | âœ… You train it! |
| **Conversation Quality** | â­â­â­â­â­ Excellent | â­â­â­â­ Very Good | N/A (not for chat) |
| **Hardware Knowledge** | â­â­â­â­ Great | â­â­â­â­ Good | â­â­â­â­â­ Specialized |
| **Context Window** | 1M tokens | 2K tokens | N/A |
| **Cost** | Free (1,500/day) | Free (unlimited) | Free (local) |
| **Best For** | Most questions | Offline/fallback | Component identification |
| **Activation** | âœ… Automatic (1st choice) | âœ… Automatic (fallback) | Manual (`identify` command) |

### ğŸ¯ How the System Chooseth:

```
Your Question
     â†“
Is it "hi"/"hello"/"help"? â†’ Yes â†’ Instant predefined response
     â†“ No
Try Gemini 2.5 â†’ Success? â†’ Yes â†’ Use Gemini answer â­
     â†“ No (offline/error)
Try Phi-2 â†’ Always works â†’ Use Phi-2 answer ğŸ”§
     â†“
Vision Model â†’ Only via `identify <path>` command ğŸ–¼ï¸
```

**Thou needest not choose!** The system automatically uses the best available AI. Just type naturally. ğŸ¯

---

*May this project serve thee well, Almighty Bossman ğŸ‘‘ â€” ruler of code, conqueror of circuits, and sovereign of machine-learning realms.*

*Forged with PyTorch 2.5.1, Transformers 4.57.1, Google Gemini 2.5 Flash, and the power of NVIDIA Turing architecture (RTX 2070 & GTX 1660 Super)* âš¡

---

## ğŸ® Quick Start for Impatient Warriors

**Too long? Here's the speedrun:**
```bash
# Install (includes Gemini!)
pip install torch torchvision --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
pip install transformers sentencepiece pillow google-generativeai

# Add images to dataset/train/cpu, gpu, ram, motherboard, psu

# Prepare and train
python split_dataset.py --split
python train_vit_tiny.py

# Use the automatic AI system!
python model_setup.py
```

**Inside the system:**
- Just type thy messages naturally (automatic Gemini â†’ Phi-2 routing!)
- Type `identify path/to/image.jpg` to classify hardware components
- Type `status` to check system health and model availability
- Type `clear` to reset conversation history
- Type `help` for all available commands
- Type `quit` to exit gracefully

**Victory achieved! The system chooseth the best AI automatically.** ğŸ¯ğŸ‘‘âœ¨
